{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ba4f65f",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Практическое-упражнение:\" data-toc-modified-id=\"Практическое-упражнение:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Практическое упражнение:</a></span></li><li><span><a href=\"#Обучение-моделей:\" data-toc-modified-id=\"Обучение-моделей:-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение моделей:</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150dd663",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T16:16:49.557604Z",
     "start_time": "2023-08-07T16:16:49.466579Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1789d6dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T16:16:49.588410Z",
     "start_time": "2023-08-07T16:16:49.559602Z"
    }
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Node class will hold information about one node in the decision tree.\"\"\"\n",
    "\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        \"\"\"\n",
    "        Initialization of decision node.\n",
    "\n",
    "        :param int feature_index: Index of feature by which data was split\n",
    "        :param float threshold: Threshold value by which data was split\n",
    "        :param Node left: Left child node\n",
    "        :param Node right: Right child node\n",
    "        :param float info_gain: Information gain of the split\n",
    "        :param float value: Value if the node is a leaf in the tree\n",
    "        \"\"\"\n",
    "\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.info_gain = info_gain\n",
    "        self.value = value\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"Decision tree class for binary classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        \"\"\"\n",
    "        Initialization of decision tree classifier.\n",
    "\n",
    "        :param int min_samples_split: The minimum number of samples required to split an internal node\n",
    "        :param int max_depth: The maximum depth of the tree\n",
    "        \"\"\"\n",
    "\n",
    "        self.root = None\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        \"\"\"\n",
    "        Calculate entropy of the labels.\n",
    "\n",
    "        :param ndarray y: Target values\n",
    "        :return: entropy\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "\n",
    "        classes = np.unique(y)\n",
    "        entropy = 0\n",
    "        for cls in classes:\n",
    "            p_cls = len(y[y == cls]) / len(y)\n",
    "            entropy += -p_cls * np.log2(p_cls)\n",
    "        return entropy\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find best feature and value for a split. Greedy algorithm.\n",
    "\n",
    "        :param ndarray X: Feature dataset\n",
    "        :param ndarray y: Target values\n",
    "        :return: node information gain, feature index and threshold\n",
    "        :rtype: tuple\n",
    "        \"\"\"\n",
    "\n",
    "        best_split = {}\n",
    "        max_info_gain = -1\n",
    "        n_rows, n_cols = X.shape\n",
    "        for f_idx in range(n_cols):\n",
    "            X_curr = X[:, f_idx]\n",
    "            for threshold in np.unique(X_curr):\n",
    "                gain = self.information_gain(y, X_curr, threshold)\n",
    "                if gain > max_info_gain:\n",
    "                    max_info_gain = gain\n",
    "                    best_split = {\"feature_index\": f_idx, \"threshold\": threshold, \"info_gain\": gain}\n",
    "        return best_split\n",
    "\n",
    "    def information_gain(self, y, X_col, split_thresh):\n",
    "        \"\"\"\n",
    "        Compute information gain of a split.\n",
    "\n",
    "        :param ndarray y: Target values\n",
    "        :param ndarray X_col: Feature column values\n",
    "        :param float split_thresh: Value to split the feature column\n",
    "        :return: information gain of the split\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "\n",
    "        parent_entropy = self.entropy(y)\n",
    "\n",
    "        # Generate split\n",
    "        left_idx = np.where(X_col < split_thresh)\n",
    "        right_idx = np.where(X_col >= split_thresh)\n",
    "\n",
    "        if len(left_idx[0]) == 0 or len(right_idx[0]) == 0:  # Skip if split not feasible\n",
    "            return -1\n",
    "\n",
    "        # Compute children entropy\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idx[0]), len(right_idx[0])\n",
    "        e_l, e_r = self.entropy(y[left_idx]), self.entropy(y[right_idx])\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # Compute information gain\n",
    "        ig = parent_entropy - child_entropy\n",
    "        return ig\n",
    "\n",
    "    def build_tree(self, X, y, curr_depth=0):\n",
    "        \"\"\"\n",
    "        Recursive method to create the decision tree.\n",
    "\n",
    "        :param ndarray X: Feature dataset\n",
    "        :param ndarray y: Target values\n",
    "        :param int curr_depth: Current depth level\n",
    "        :return: constructed node\n",
    "        :rtype: Node\n",
    "        \"\"\"\n",
    "\n",
    "        n_rows, n_cols = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # Stop criteria\n",
    "        if n_rows >= self.min_samples_split and n_labels > 1 and curr_depth <= self.max_depth:\n",
    "            best = self.best_split(X, y)\n",
    "            if best[\"info_gain\"] > 0:\n",
    "                left_subtree = self.build_tree(X[X[:, best[\"feature_index\"]] < best[\"threshold\"], :], y[X[:, best[\"feature_index\"]] < best[\"threshold\"]], curr_depth + 1)\n",
    "                right_subtree = self.build_tree(X[X[:, best[\"feature_index\"]] >= best[\"threshold\"], :], y[X[:, best[\"feature_index\"]] >= best[\"threshold\"]], curr_depth + 1)\n",
    "                return Node(best[\"feature_index\"], best[\"threshold\"], left_subtree, right_subtree, best[\"info_gain\"])\n",
    "        \n",
    "        leaf_value = self.majority_vote(y)\n",
    "        return Node(value=leaf_value)\n",
    "\n",
    "    @staticmethod\n",
    "    def majority_vote(y):\n",
    "        \"\"\"\n",
    "        Return the majority voted class.\n",
    "\n",
    "        :param ndarray y: Target values\n",
    "        :return: majority class\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "\n",
    "        most_common = np.argmax(np.bincount(y))\n",
    "        return most_common\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \"\"\"\n",
    "        Recursive print of decision tree structure.\n",
    "\n",
    "        :param Node tree: Tree object\n",
    "        :param str indent: Indentation string\n",
    "        \"\"\"\n",
    "\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "        else:\n",
    "            print(f'X_{tree.feature_index} <= {tree.threshold}')\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right, indent + indent)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build decision tree classifier.\n",
    "\n",
    "        :param ndarray X: Feature dataset\n",
    "        :param ndarray y: Target values\n",
    "        \"\"\"\n",
    "\n",
    "        self.root = self.build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions of target values with the fitted decision tree.\n",
    "\n",
    "        :param ndarray X: Feature dataset\n",
    "        :return: predicted values\n",
    "        :rtype: ndarray\n",
    "        \"\"\"\n",
    "\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        \"\"\"\n",
    "        Auxiliary method to make predictions.\n",
    "\n",
    "        :param list inputs: Single data instance\n",
    "        :return: predicted class\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "\n",
    "        node = self.root\n",
    "        while node.value is None:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4178fc7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T16:16:50.264571Z",
     "start_time": "2023-08-07T16:16:49.590386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "X_2 <= 3.0\n",
      " left:0\n",
      " right:X_3 <= 1.8\n",
      "  left:X_2 <= 5.0\n",
      "    left:X_3 <= 1.7\n",
      "        left:1\n",
      "        right:2\n",
      "    right:X_3 <= 1.6\n",
      "        left:2\n",
      "        right:1\n",
      "  right:X_2 <= 4.9\n",
      "    left:X_0 <= 6.0\n",
      "        left:1\n",
      "        right:2\n",
      "    right:2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "tree = DecisionTree(max_depth=3)\n",
    "tree.fit(X, y)\n",
    "\n",
    "print(tree.predict([[0, 0, 5, 1.5]]))  # пример прогноза\n",
    "\n",
    "tree.print_tree()  # вывод структуры дерева\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2930a6",
   "metadata": {},
   "source": [
    "Суть ансамблевых методов заключается в создании набора простых моделей, которые, работая вместе, обеспечивают более высокую производительность, чем каждая из моделей по отдельности. Другими словами, группа слабых моделей, объединенных вместе, может дать в совокупности более сильный и точный прогноз. Этот подход основан на поговорке \"Много глаз видят больше\".\n",
    "\n",
    "Важным преимуществом ансамблевых методов является их способность уменьшить переобучение, которое является одной из основных проблем в машинном обучении. Переобученная модель \"слишком хорошо\" подходит к тренировочным данным, но плохо обобщает новые, неизвестные данные. Ансамблевые методы могут уменьшить вероятность переобучения, поскольку они усредняют предсказания нескольких моделей, что помогает сгладить выбросы и уменьшить дисперсию.\n",
    "\n",
    "Еще одно важное преимущество ансамблевых методов - это повышение точности прогнозирования. Используя несколько моделей вместо одной, мы получаем возможность учесть больше нюансов в данных и сделать более точные прогнозы.\n",
    "\n",
    "Ансамблевые методы можно разделить на несколько основных категорий, среди которых самыми распространенными являются бэггинг и бустинг.\n",
    "\n",
    "Бэггинг, что является сокращением от \"bootstrap aggregating\", заключается в генерации нескольких подвыборок из исходного набора данных с заменой, обучении отдельной модели для каждой из подвыборок, а затем усреднении их прогнозов. Примером алгоритма, использующего бэггинг, является Random Forest.\n",
    "\n",
    "Бустинг, в свою очередь, представляет собой итеративный процесс, где каждая следующая модель стремится исправить ошибки предыдущей. Градиентный бустинг - это одна из популярных техник бустинга, которая оптимизирует функцию потерь, пытаясь сделать больший упор на неправильно классифицированных примерах на предыдущих итерациях."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21650bd9",
   "metadata": {},
   "source": [
    "Бэггинг, или bootstrap aggregating, это метод, который генерирует несколько подвыборок из исходного набора данных с заменой и обучает модель на каждом из этих подмножеств. Затем берутся прогнозы каждой обученной модели и происходит их агрегация, или усреднение. Это означает, что для задачи регрессии мы просто усредним прогнозы, а для задачи классификации мы выберем класс, который получил большинство голосов.\n",
    "\n",
    "Основное преимущество бэггинга заключается в уменьшении дисперсии модели. Дисперсия описывает, насколько предсказания модели изменяются для данного наблюдаемого значения. В контексте машинного обучения высокая дисперсия может указывать на переобучение, когда модель слишком хорошо подходит к обучающим данным и плохо обобщает на новые данные. Бэггинг уменьшает дисперсию, генерируя большое количество подмножеств данных и обучая модель на каждом из них. Это позволяет учесть различные аспекты данных и сгладить выбросы.\n",
    "\n",
    "Давайте рассмотрим пример применения бэггинга: алгоритм случайного леса. Random Forest - это мощный алгоритм машинного обучения, который использует бэггинг в сочетании с деревьями решений. Для каждого дерева в лесу создается подвыборка обучающих данных с заменой, а затем дерево обучается на этом подмножестве. Важно отметить, что при каждом разделении дерева используется случайный поднабор признаков, что добавляет дополнительную стохастичность в модель и помогает уменьшить корреляцию между деревьями."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c4ac9d",
   "metadata": {},
   "source": [
    "Давайте перейдем к примерам использования бэггинга. Этот метод широко используется во многих областях, где требуется машинное обучение.\n",
    "\n",
    "Прогнозирование в области финансов: Бэггинг может быть использован для прогнозирования финансовых рынков. В этом случае несколько моделей обучаются на различных подмножествах финансовых данных и их прогнозы агрегируются для получения конечного прогноза.\n",
    "\n",
    "Медицинская диагностика: Бэггинг также используется в медицинской диагностике, например, для обнаружения заболеваний на основе медицинских изображений. Несколько моделей обучаются на различных подмножествах данных и их прогнозы агрегируются для получения конечного диагноза.\n",
    "\n",
    "Биология и экология: В этих областях бэггинг используется для классификации и идентификации видов на основе их характеристик.\n",
    "\n",
    "Следует отметить, что бэггинг часто используется в сочетании с алгоритмами машинного обучения, основанными на деревьях решений. Деревья решений являются хорошим выбором для бэггинга, поскольку они могут обучаться на разных подмножествах данных и могут создавать сложные модели.\n",
    "\n",
    "Теперь давайте рассмотрим, как можно применить бэггинг с использованием библиотеки Scikit-Learn. Мы будем использовать BaggingClassifier, который предоставляет все необходимые инструменты для реализации бэггинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0854fe50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T16:16:50.633571Z",
     "start_time": "2023-08-07T16:16:50.267575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier accuracy:  0.8733333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sidromnik\\.conda\\envs\\TeachMeSkills\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Создание искусственного набора данных\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Инициализация базового классификатора\n",
    "base_cls = KNeighborsClassifier()\n",
    "\n",
    "# Инициализация BaggingClassifier с KNeighborsClassifier в качестве базового классификатора\n",
    "bag_cls = BaggingClassifier(base_estimator=base_cls, n_estimators=10, random_state=42)\n",
    "\n",
    "# Обучение BaggingClassifier\n",
    "bag_cls.fit(X_train, y_train)\n",
    "\n",
    "# Оценка BaggingClassifier\n",
    "print(\"BaggingClassifier accuracy: \", bag_cls.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bbb803",
   "metadata": {},
   "source": [
    "\"Бустинг\" является другим важным ансамблевым методом. Он работает последовательно, то есть каждая последующая модель в ансамбле стремится исправить ошибки предыдущей.\n",
    "\n",
    "Модели создаются последовательно, где каждая следующая модель учится исправлять ошибки предыдущей. Это происходит за счет концентрации на тех примерах обучающего набора, которые были классифицированы неправильно или которые представляют больший вызов для модели.\n",
    "\n",
    "Основное преимущество бустинга - это его способность увеличивать точность модели. Однако это также может привести к риску переобучения, если не ограничить сложность базовых моделей или не использовать регуляризацию.\n",
    "\n",
    "Примером алгоритма бустинга является градиентный бустинг, который обычно использует деревья решений в качестве базовых моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2260533",
   "metadata": {},
   "source": [
    "Бустинг широко используется в многих приложениях, где требуется максимальная точность прогнозов.\n",
    "\n",
    "Прогнозирование в области финансов: Бустинг используется для прогнозирования цен акций, валютных курсов и других финансовых показателей.\n",
    "\n",
    "Медицинская диагностика: Бустинг может быть использован для обнаружения заболеваний на основе медицинских изображений или данных пациентов.\n",
    "\n",
    "Обработка естественного языка (NLP): Бустинг также используется в NLP для классификации текстов и анализа настроений.\n",
    "\n",
    "Теперь давайте рассмотрим, как можно применить бустинг с использованием библиотеки Scikit-Learn. Мы будем использовать GradientBoostingClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd531e00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T16:16:51.237585Z",
     "start_time": "2023-08-07T16:16:50.634572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier accuracy:  0.8866666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Создание искусственного набора данных\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Инициализация GradientBoostingClassifier\n",
    "gb_cls = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Обучение GradientBoostingClassifier\n",
    "gb_cls.fit(X_train, y_train)\n",
    "\n",
    "# Оценка GradientBoostingClassifier\n",
    "print(\"GradientBoostingClassifier accuracy: \", gb_cls.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3709acb",
   "metadata": {},
   "source": [
    "## Практическое упражнение:\n",
    "Реализация бэггинга и бустинга с использованием Python и Scikit-Learn\n",
    "\n",
    "Задача: В данном упражнении мы будем использовать классификаторы Bagging и Gradient Boosting для решения задачи классификации и сравним их результаты с результатами, полученными от отдельных деревьев решений.\n",
    "\n",
    "1. Подготовка данных:\n",
    "\n",
    "Для упражнения мы будем использовать датасет Iris из библиотеки sklearn. Загрузим датасет и разделим его на обучающую и тестовую выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4089a57c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T16:16:51.253573Z",
     "start_time": "2023-08-07T16:16:51.238571Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21443ee2",
   "metadata": {},
   "source": [
    "## Обучение моделей:\n",
    "\n",
    "Сначала обучим одиночное дерево решений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1668a1bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T16:16:51.269582Z",
     "start_time": "2023-08-07T16:16:51.254573Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Decision Tree accuracy: \", tree.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e58f30",
   "metadata": {},
   "source": [
    "Теперь применим бэггинг:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5987cdc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T16:16:51.423576Z",
     "start_time": "2023-08-07T16:16:51.271574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging accuracy:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sidromnik\\.conda\\envs\\TeachMeSkills\\lib\\site-packages\\sklearn\\ensemble\\_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "print(\"Bagging accuracy: \", bag_clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e302a928",
   "metadata": {},
   "source": [
    "И наконец, применим бустинг:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "048b1d10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T16:16:51.644572Z",
     "start_time": "2023-08-07T16:16:51.425575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "boost_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "boost_clf.fit(X_train, y_train)\n",
    "print(\"Boosting accuracy: \", boost_clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de782086",
   "metadata": {},
   "source": [
    "3. Сравнение результатов:\n",
    "\n",
    "После выполнения кода каждый из студентов должен увидеть результаты точности для каждой из моделей. Это даст возможность сравнить эффективность каждого из методов на данной задаче классификации.\n",
    "\n",
    "Важно отметить, что результаты могут варьироваться в зависимости от набора данных и параметров модели. Бустинг и бэггинг обычно обеспечивают лучшую обобщающую способность, чем одиночные деревья решений, но они также могут быть более подвержены переобучению, если не ограничивать сложность базовых моделей или не использовать регуляризацию.\n",
    "\n",
    "При выполнении этого упражнения студенты могут экспериментировать с различными параметрами, такими как количество оценщиков (n_estimators) и коэффициент обучения (learning_rate), чтобы увидеть, как они влияют на результаты моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab0039c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a0af31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b890051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b5b9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b253bf89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
