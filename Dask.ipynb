{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных (в качестве примера взята часть данных за 2008 год)\n",
    "df = dd.read_csv('2008/2008.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 29 entries, Year to LateAircraftDelay\n",
      "dtypes: object(4), float64(6), int64(19)"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = {\n",
    "    'ActualElapsedTime': 'float64',\n",
    "    'AirTime': 'float64',\n",
    "    'ArrDelay': 'float64',\n",
    "    'ArrTime': 'float64',\n",
    "    'CRSElapsedTime': 'float64',\n",
    "    'CancellationCode': 'object',\n",
    "    'DepDelay': 'float64',\n",
    "    'DepTime': 'float64',\n",
    "    'TaxiIn': 'float64',\n",
    "    'TaxiOut': 'float64'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>...</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>CancellationCode</th>\n",
       "      <th>Diverted</th>\n",
       "      <th>CarrierDelay</th>\n",
       "      <th>WeatherDelay</th>\n",
       "      <th>NASDelay</th>\n",
       "      <th>SecurityDelay</th>\n",
       "      <th>LateAircraftDelay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1343.0</td>\n",
       "      <td>1325</td>\n",
       "      <td>1451.0</td>\n",
       "      <td>1435</td>\n",
       "      <td>WN</td>\n",
       "      <td>588</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>1120</td>\n",
       "      <td>1247.0</td>\n",
       "      <td>1245</td>\n",
       "      <td>WN</td>\n",
       "      <td>1343</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>2136.0</td>\n",
       "      <td>2140</td>\n",
       "      <td>WN</td>\n",
       "      <td>3841</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>903.0</td>\n",
       "      <td>855</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>1205</td>\n",
       "      <td>WN</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1423.0</td>\n",
       "      <td>1400</td>\n",
       "      <td>1726.0</td>\n",
       "      <td>1710</td>\n",
       "      <td>WN</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Month  DayofMonth  DayOfWeek  DepTime  CRSDepTime  ArrTime  \\\n",
       "0  2008      1           3          4   1343.0        1325   1451.0   \n",
       "1  2008      1           3          4   1125.0        1120   1247.0   \n",
       "2  2008      1           3          4   2009.0        2015   2136.0   \n",
       "3  2008      1           3          4    903.0         855   1203.0   \n",
       "4  2008      1           3          4   1423.0        1400   1726.0   \n",
       "\n",
       "   CRSArrTime UniqueCarrier  FlightNum  ... TaxiIn  TaxiOut  Cancelled  \\\n",
       "0        1435            WN        588  ...    4.0      9.0          0   \n",
       "1        1245            WN       1343  ...    3.0      8.0          0   \n",
       "2        2140            WN       3841  ...    2.0     14.0          0   \n",
       "3        1205            WN          3  ...    5.0      7.0          0   \n",
       "4        1710            WN         25  ...    6.0     10.0          0   \n",
       "\n",
       "   CancellationCode  Diverted  CarrierDelay WeatherDelay NASDelay  \\\n",
       "0               NaN         0          16.0          0.0      0.0   \n",
       "1               NaN         0           NaN          NaN      NaN   \n",
       "2               NaN         0           NaN          NaN      NaN   \n",
       "3               NaN         0           NaN          NaN      NaN   \n",
       "4               NaN         0          16.0          0.0      0.0   \n",
       "\n",
       "   SecurityDelay  LateAircraftDelay  \n",
       "0            0.0                0.0  \n",
       "1            NaN                NaN  \n",
       "2            NaN                NaN  \n",
       "3            NaN                NaN  \n",
       "4            0.0                0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dd.read_csv('2008/2008.csv', dtype=dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['UniqueCarrier', 'Origin', 'Dest', 'CancellationCode', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay', 'TailNum'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>CRSElapsedTime</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>Distance</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>794467</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>1250</td>\n",
       "      <td>1553.0</td>\n",
       "      <td>1536</td>\n",
       "      <td>1194</td>\n",
       "      <td>109.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>588</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794468</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1655.0</td>\n",
       "      <td>1655</td>\n",
       "      <td>2340.0</td>\n",
       "      <td>2333</td>\n",
       "      <td>1194</td>\n",
       "      <td>285.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2105</td>\n",
       "      <td>10.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794469</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1747.0</td>\n",
       "      <td>1750</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>2024</td>\n",
       "      <td>1195</td>\n",
       "      <td>265.0</td>\n",
       "      <td>274.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1864</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794470</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2128.0</td>\n",
       "      <td>2131</td>\n",
       "      <td>2244.0</td>\n",
       "      <td>2232</td>\n",
       "      <td>1195</td>\n",
       "      <td>136.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>588</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794471</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1117.0</td>\n",
       "      <td>1120</td>\n",
       "      <td>1348.0</td>\n",
       "      <td>1352</td>\n",
       "      <td>1197</td>\n",
       "      <td>391.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>2936</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794472</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1844.0</td>\n",
       "      <td>1845</td>\n",
       "      <td>2126.0</td>\n",
       "      <td>2130</td>\n",
       "      <td>1198</td>\n",
       "      <td>162.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>950</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794473</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>1200</td>\n",
       "      <td>1335.0</td>\n",
       "      <td>1337</td>\n",
       "      <td>1199</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>445</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794474</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1441.0</td>\n",
       "      <td>1445</td>\n",
       "      <td>1632.0</td>\n",
       "      <td>1635</td>\n",
       "      <td>1200</td>\n",
       "      <td>111.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>576</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794475</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>612.0</td>\n",
       "      <td>615</td>\n",
       "      <td>811.0</td>\n",
       "      <td>819</td>\n",
       "      <td>1201</td>\n",
       "      <td>119.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>661</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794476</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>1940</td>\n",
       "      <td>2128.0</td>\n",
       "      <td>2135</td>\n",
       "      <td>1202</td>\n",
       "      <td>108.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>661</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794477</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>712.0</td>\n",
       "      <td>720</td>\n",
       "      <td>823.0</td>\n",
       "      <td>844</td>\n",
       "      <td>1203</td>\n",
       "      <td>71.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>306</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794478</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>934.0</td>\n",
       "      <td>940</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>1050</td>\n",
       "      <td>1203</td>\n",
       "      <td>122.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>692</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794479</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>739.0</td>\n",
       "      <td>740</td>\n",
       "      <td>951.0</td>\n",
       "      <td>945</td>\n",
       "      <td>1204</td>\n",
       "      <td>132.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>752</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794480</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>601.0</td>\n",
       "      <td>600</td>\n",
       "      <td>823.0</td>\n",
       "      <td>819</td>\n",
       "      <td>1205</td>\n",
       "      <td>142.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>752</td>\n",
       "      <td>8.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794481</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1047.0</td>\n",
       "      <td>1050</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>1252</td>\n",
       "      <td>1206</td>\n",
       "      <td>121.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>752</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794482</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1025.0</td>\n",
       "      <td>1025</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>1237</td>\n",
       "      <td>1207</td>\n",
       "      <td>129.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>752</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794483</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1319.0</td>\n",
       "      <td>1320</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>1524</td>\n",
       "      <td>1208</td>\n",
       "      <td>128.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>752</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794484</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1335.0</td>\n",
       "      <td>1335</td>\n",
       "      <td>1556.0</td>\n",
       "      <td>1553</td>\n",
       "      <td>1209</td>\n",
       "      <td>141.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>752</td>\n",
       "      <td>7.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794485</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1933.0</td>\n",
       "      <td>1935</td>\n",
       "      <td>2140.0</td>\n",
       "      <td>2141</td>\n",
       "      <td>1210</td>\n",
       "      <td>127.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>752</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794486</th>\n",
       "      <td>2008</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>621.0</td>\n",
       "      <td>615</td>\n",
       "      <td>752.0</td>\n",
       "      <td>754</td>\n",
       "      <td>1211</td>\n",
       "      <td>91.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>430</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Year  Month  DayofMonth  DayOfWeek  DepTime  CRSDepTime  ArrTime  \\\n",
       "794467  2008      4          17          4   1304.0        1250   1553.0   \n",
       "794468  2008      4          17          4   1655.0        1655   2340.0   \n",
       "794469  2008      4          17          4   1747.0        1750   2012.0   \n",
       "794470  2008      4          17          4   2128.0        2131   2244.0   \n",
       "794471  2008      4          17          4   1117.0        1120   1348.0   \n",
       "794472  2008      4          17          4   1844.0        1845   2126.0   \n",
       "794473  2008      4          17          4   1157.0        1200   1335.0   \n",
       "794474  2008      4          17          4   1441.0        1445   1632.0   \n",
       "794475  2008      4          17          4    612.0         615    811.0   \n",
       "794476  2008      4          17          4   1940.0        1940   2128.0   \n",
       "794477  2008      4          17          4    712.0         720    823.0   \n",
       "794478  2008      4          17          4    934.0         940   1036.0   \n",
       "794479  2008      4          17          4    739.0         740    951.0   \n",
       "794480  2008      4          17          4    601.0         600    823.0   \n",
       "794481  2008      4          17          4   1047.0        1050   1248.0   \n",
       "794482  2008      4          17          4   1025.0        1025   1234.0   \n",
       "794483  2008      4          17          4   1319.0        1320   1527.0   \n",
       "794484  2008      4          17          4   1335.0        1335   1556.0   \n",
       "794485  2008      4          17          4   1933.0        1935   2140.0   \n",
       "794486  2008      4          17          4    621.0         615    752.0   \n",
       "\n",
       "        CRSArrTime  FlightNum  ActualElapsedTime  CRSElapsedTime  AirTime  \\\n",
       "794467        1536       1194              109.0           106.0     85.0   \n",
       "794468        2333       1194              285.0           278.0    252.0   \n",
       "794469        2024       1195              265.0           274.0    246.0   \n",
       "794470        2232       1195              136.0           121.0    103.0   \n",
       "794471        1352       1197              391.0           392.0    362.0   \n",
       "794472        2130       1198              162.0           165.0    138.0   \n",
       "794473        1337       1199               98.0            97.0     74.0   \n",
       "794474        1635       1200              111.0           110.0     89.0   \n",
       "794475         819       1201              119.0           124.0     99.0   \n",
       "794476        2135       1202              108.0           115.0     91.0   \n",
       "794477         844       1203               71.0            84.0     50.0   \n",
       "794478        1050       1203              122.0           130.0    102.0   \n",
       "794479         945       1204              132.0           125.0    102.0   \n",
       "794480         819       1205              142.0           139.0    106.0   \n",
       "794481        1252       1206              121.0           122.0    101.0   \n",
       "794482        1237       1207              129.0           132.0    108.0   \n",
       "794483        1524       1208              128.0           124.0    107.0   \n",
       "794484        1553       1209              141.0           138.0    103.0   \n",
       "794485        2141       1210              127.0           126.0    106.0   \n",
       "794486         754       1211               91.0            99.0     64.0   \n",
       "\n",
       "        ArrDelay  DepDelay  Distance  TaxiIn  TaxiOut  Cancelled  Diverted  \n",
       "794467      17.0      14.0       588     8.0     16.0          0         0  \n",
       "794468       7.0       0.0      2105    10.0     23.0          0         0  \n",
       "794469     -12.0      -3.0      1864     8.0     11.0          0         0  \n",
       "794470      12.0      -3.0       588     6.0     27.0          0         0  \n",
       "794471      -4.0      -3.0      2936     5.0     24.0          0         0  \n",
       "794472      -4.0      -1.0       950    15.0      9.0          0         0  \n",
       "794473      -2.0      -3.0       445    11.0     13.0          0         0  \n",
       "794474      -3.0      -4.0       576     6.0     16.0          0         0  \n",
       "794475      -8.0      -3.0       661     5.0     15.0          0         0  \n",
       "794476      -7.0       0.0       661     7.0     10.0          0         0  \n",
       "794477     -21.0      -8.0       306     8.0     13.0          0         0  \n",
       "794478     -14.0      -6.0       692     4.0     16.0          0         0  \n",
       "794479       6.0      -1.0       752     6.0     24.0          0         0  \n",
       "794480       4.0       1.0       752     8.0     28.0          0         0  \n",
       "794481      -4.0      -3.0       752     8.0     12.0          0         0  \n",
       "794482      -3.0       0.0       752     5.0     16.0          0         0  \n",
       "794483       3.0      -1.0       752     9.0     12.0          0         0  \n",
       "794484       3.0       0.0       752     7.0     31.0          0         0  \n",
       "794485      -1.0      -2.0       752     9.0     12.0          0         0  \n",
       "794486      -2.0       6.0       430    15.0     12.0          0         0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2389217 entries, 0 to 794486\n",
      "Data columns (total 19 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   Year               int64  \n",
      " 1   Month              int64  \n",
      " 2   DayofMonth         int64  \n",
      " 3   DayOfWeek          int64  \n",
      " 4   DepTime            float64\n",
      " 5   CRSDepTime         int64  \n",
      " 6   ArrTime            float64\n",
      " 7   CRSArrTime         int64  \n",
      " 8   FlightNum          int64  \n",
      " 9   ActualElapsedTime  float64\n",
      " 10  CRSElapsedTime     float64\n",
      " 11  AirTime            float64\n",
      " 12  ArrDelay           float64\n",
      " 13  DepDelay           float64\n",
      " 14  Distance           int64  \n",
      " 15  TaxiIn             float64\n",
      " 16  TaxiOut            float64\n",
      " 17  Cancelled          int64  \n",
      " 18  Diverted           int64  \n",
      "dtypes: float64(9), int64(10)\n",
      "memory usage: 364.6 MB\n"
     ]
    }
   ],
   "source": [
    "df.compute().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее время задержки вылета: 11.44 минут\n",
      "Количество отмененных рейсов: 64442\n",
      "Топ-5 маршрутов с максимальными задержками:\n",
      "Origin  Dest\n",
      "SBN     CVG     518.0\n",
      "SDF     SPI     329.0\n",
      "HPN     PIA     298.0\n",
      "TUL     PIA     243.0\n",
      "ONT     SAN     221.0\n",
      "Name: DepDelay, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Вычисляем среднее время задержки вылета\n",
    "mean_delay = df['DepDelay'].mean().compute()\n",
    "print(f\"Среднее время задержки вылета: {mean_delay:.2f} минут\")\n",
    "\n",
    "# Количество отмененных рейсов\n",
    "cancelled_flights = df[df['Cancelled'] == 1].shape[0].compute()\n",
    "print(f\"Количество отмененных рейсов: {cancelled_flights}\")\n",
    "\n",
    "# Топ-5 маршрутов с максимальными задержками\n",
    "top_routes = df.groupby(['Origin', 'Dest'])['DepDelay'].mean().nlargest(5).compute()\n",
    "print(\"Топ-5 маршрутов с максимальными задержками:\")\n",
    "print(top_routes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['DayOfWeek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.array.core.Array"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop('DayOfWeek', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.array.core.Array"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to_dask_array()\n",
    "y = y.to_dask_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2389217, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2389217,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 318.48 MiB </td>\n",
       "                        <td> 106.64 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (2319121, 18) </td>\n",
       "                        <td> (776558, 18) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 3 chunks in 6 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"75\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"25\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"39\" x2=\"25\" y2=\"39\" />\n",
       "  <line x1=\"0\" y1=\"79\" x2=\"25\" y2=\"79\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"25\" y1=\"0\" x2=\"25\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 25.412616514582485,0.0 25.412616514582485,120.0 0.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"12.706308\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >18</text>\n",
       "  <text x=\"45.412617\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,45.412617,60.000000)\">2319121</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<values, shape=(2319121, 18), dtype=float64, chunksize=(776558, 18), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.compute_chunk_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 17.69 MiB </td>\n",
       "                        <td> 5.92 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (2319121,) </td>\n",
       "                        <td> (776558,) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 3 chunks in 6 graph layers </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> int64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"39\" y1=\"0\" x2=\"39\" y2=\"25\" />\n",
       "  <line x1=\"79\" y1=\"0\" x2=\"79\" y2=\"25\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.0,0.0 120.0,0.0 120.0,25.412616514582485 0.0,25.412616514582485\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >2319121</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<values, shape=(2319121,), dtype=int64, chunksize=(776558,), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.compute_chunk_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4\n",
       "1    5\n",
       "2    6\n",
       "3    7\n",
       "4    1\n",
       "5    2\n",
       "6    3\n",
       "Name: DayOfWeek, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DayOfWeek'].unique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 200804   55700  140489 ... 2212728 2038685 1899159]\n",
      "[ 112116  718681  116671 ... 2332924 1706625 1990523]\n",
      "[ 766376  225804  400088 ... 2116739 2125428 2331722]\n",
      "[ 254924  169416  120110 ... 1954901 2034181 2023407]\n",
      "[ 241991  602758  471322 ... 1813962 2165224 2153725]\n"
     ]
    }
   ],
   "source": [
    "from dask_ml.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=5, test_size=0.3, train_size=0.7, random_state=0)\n",
    "for train_idx, test_idx in cv.split(X=x, y=y):\n",
    "    print(train_idx.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 463825  463826  463827 ... 2319118 2319119 2319120]\n",
      "[      0       1       2 ... 2319118 2319119 2319120]\n",
      "[      0       1       2 ... 2319118 2319119 2319120]\n",
      "[      0       1       2 ... 2319118 2319119 2319120]\n",
      "[      0       1       2 ... 1855294 1855295 1855296]\n"
     ]
    }
   ],
   "source": [
    "from dask_ml.model_selection import KFold\n",
    "cv = KFold(n_splits=5, random_state=0)\n",
    "\n",
    "for train_idx, test_idx in cv.split(X=x, y=y):\n",
    "    print(train_idx.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gea/.local/lib/python3.8/site-packages/dask_glm/utils.py:52: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(A)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver_kwargs={&#x27;normalize&#x27;: False})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver_kwargs={&#x27;normalize&#x27;: False})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver_kwargs={'normalize': False})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask_ml.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver_kwargs={\"normalize\":False})\n",
    "lr.fit(x.compute(), y.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import joblib\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers = 4, threads_per_worker = 1, memory_limit = '4GB')\n",
    "\n",
    "\n",
    "# 2. Выбор модели\n",
    "model = LogisticRegression(max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средний результат на кросс-валидации: 0.16\n",
      "Стандартное отклонение: 0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import joblib\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers = 4, threads_per_worker = 1, memory_limit = '4GB')\n",
    "\n",
    "\n",
    "# 2. Выбор модели\n",
    "model = LogisticRegression(max_iter=200)\n",
    "with parallel_backend('dask'):\n",
    "    model.fit(x, y)\n",
    "    k = 5\n",
    "    scores = cross_val_score(model, x, y, cv=k)\n",
    "    mean_score = np.mean(scores)\n",
    "    std_deviation = np.std(scores)\n",
    "    print(f\"Средний результат на кросс-валидации: {mean_score:.2f}\")\n",
    "    print(f\"Стандартное отклонение: {std_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Fold CV Scores: [0.15971325 0.15202103 0.14817689 0.14711615 0.16047251]\n",
      "Average Score: 0.1534999656278011\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "with parallel_backend('dask'):\n",
    "    kf = KFold(n_splits=5)\n",
    "    scores = cross_val_score(model, x, y, cv=kf)\n",
    "\n",
    "    print(\"K-Fold CV Scores:\", scores)\n",
    "    print(\"Average Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified K-Fold CV Scores: [0.15730071 0.1525169  0.16601771 0.15146262 0.15563231]\n",
      "Average Score: 0.15658605104491957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "with parallel_backend('dask'):\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    scores = cross_val_score(model, x, y, cv=skf)\n",
    "\n",
    "    print(\"Stratified K-Fold CV Scores:\", scores)\n",
    "    print(\"Average Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series Split CV Scores: [0.15833333 0.14405723 0.13754269 0.14906603 0.16175877]\n",
      "Average Score: 0.15015160923108764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "with parallel_backend('dask'):\n",
    "    # Этот пример может не быть совсем релевантным для датасета Iris, так как это не временной ряд.\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    scores = cross_val_score(model, x, y, cv=tscv)\n",
    "\n",
    "    print(\"Time Series Split CV Scores:\", scores)\n",
    "    print(\"Average Score:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2023-08-28 11:13:17,245 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:13:19,150 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:13:19,190 - distributed.worker.memory - WARNING - Worker is at 58% memory usage. Resuming worker. Process memory: 2.20 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:13:30,648 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:13:46,008 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:13:57,709 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.60 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:13:59,324 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:13:59,450 - distributed.worker.memory - WARNING - Worker is at 67% memory usage. Resuming worker. Process memory: 2.51 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:01,390 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:01,490 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.86 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:01,890 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:02,117 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:02,389 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:02,589 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:03,091 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:03,390 - distributed.worker.memory - WARNING - Worker is at 72% memory usage. Resuming worker. Process memory: 2.70 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:03,591 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:03,690 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:04,090 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:04,327 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:04,591 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:04,889 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.90 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:05,190 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:05,499 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:05,691 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:05,890 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:06,390 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:06,717 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:06,991 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:07,189 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:07,590 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.13 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:07,807 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:07,814 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:07,990 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:08,190 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:08,491 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:08,747 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:08,990 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:09,191 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.80 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:09,489 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:09,758 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:09,990 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:10,189 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.80 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:10,590 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:10,831 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:11,090 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:11,289 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.80 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:11,590 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.17 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:11,746 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:11,990 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:12,189 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.91 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:12,489 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:12,646 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:12,889 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:13,091 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.90 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:13,391 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:13,613 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:13,891 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:14,090 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:14,489 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:14,677 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:14,990 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:15,090 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:15,490 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.15 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:15,649 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:15,889 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:16,090 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:16,489 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:16,725 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:16,990 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:17,190 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:17,490 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:17,705 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:17,890 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:17,892 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:18,190 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:18,590 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:18,884 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:19,190 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.98 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:19,489 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.91 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:19,890 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:20,144 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:20,489 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:20,689 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.91 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:20,990 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:21,297 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:21,590 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:21,789 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:22,191 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:22,381 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:22,690 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:22,891 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:23,289 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:23,525 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:23,889 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:24,091 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:24,491 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:24,761 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:25,089 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:25,289 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:25,691 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:25,949 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:26,289 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:26,493 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.80 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:26,989 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.13 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:27,195 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:27,490 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:27,690 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:27,893 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:27,990 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:28,209 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:28,490 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:28,691 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:29,092 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.15 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:29,260 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:29,590 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:29,789 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:30,190 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:30,430 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:30,690 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:30,889 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:31,291 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:31,482 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:31,791 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:31,990 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:32,490 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.21 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:32,671 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:32,989 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:33,189 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:33,490 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.16 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:33,660 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:34,337 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:34,390 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.86 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:34,790 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:35,050 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:35,290 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:35,591 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.80 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:35,890 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:36,179 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:36,390 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:36,589 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:36,991 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:37,237 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:37,490 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:37,691 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:37,991 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.98 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:37,994 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.98 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:38,332 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:38,590 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:38,790 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.86 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:39,190 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:39,521 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:39,789 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:39,991 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:40,390 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:40,599 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:40,891 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:41,090 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.91 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:41,390 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:41,633 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:41,889 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:42,091 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:42,490 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:42,701 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:42,990 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.98 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:43,292 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.80 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:43,591 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:43,814 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:44,089 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:44,290 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:44,689 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:44,862 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:45,089 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:45,289 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:45,689 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:45,889 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:46,190 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:46,290 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:46,691 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:46,910 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:47,189 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:47,389 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:47,791 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:48,035 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:48,036 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:48,290 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.98 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:48,490 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:48,889 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:49,070 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:49,390 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:49,591 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:49,890 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:50,116 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:50,389 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:50,491 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:50,891 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.18 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:51,060 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:51,290 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:51,491 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.80 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:51,789 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:51,965 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:52,190 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:52,390 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:52,790 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:52,993 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:53,190 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:53,389 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.80 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:53,789 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:53,981 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:54,191 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:54,390 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.86 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:54,790 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:55,056 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:55,291 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:55,491 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.90 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:55,791 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:55,986 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:56,190 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:56,389 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:56,789 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.16 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:56,988 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:57,190 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:57,390 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:57,789 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.13 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:57,984 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:58,090 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.78 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:58,190 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:58,390 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:58,790 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.98 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:59,083 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:59,389 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:59,589 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:14:59,889 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:00,196 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:00,489 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:00,689 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:00,989 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:01,226 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:01,491 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:01,690 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:02,090 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:02,281 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:02,590 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:02,791 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.91 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:03,090 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:03,337 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:03,590 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:03,789 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:04,190 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:04,402 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:04,590 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:04,789 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:05,189 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:05,424 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:05,690 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:05,889 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:06,290 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:06,562 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:06,890 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:07,089 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.80 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:07,389 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:07,610 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:07,890 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:08,089 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.86 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:08,091 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.86 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:08,489 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.15 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:08,684 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:08,990 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:09,189 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:09,491 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:09,740 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:10,090 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:10,190 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.80 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:10,589 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:10,799 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:11,090 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:11,590 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.90 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:11,890 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:12,176 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:12,491 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:12,689 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:13,090 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:13,351 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:13,590 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:13,791 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:14,190 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:14,481 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:14,691 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:14,989 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.86 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:15,389 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:15,607 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:15,789 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:15,989 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:16,491 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.18 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:16,673 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:16,889 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:17,090 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:17,489 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:17,758 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:18,090 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:18,092 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:18,189 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.80 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:18,691 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:18,958 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:19,291 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:19,391 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:19,890 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:20,127 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:20,389 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:20,590 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:20,990 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:21,242 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:21,491 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:21,690 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:22,090 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:22,329 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:22,590 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:22,891 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:23,290 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:23,526 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:23,790 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:23,990 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:24,390 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:24,618 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:24,890 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:25,091 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:25,389 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:25,647 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:25,991 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:26,191 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:26,590 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:26,785 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:27,089 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:27,291 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:27,689 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:27,879 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:28,190 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:28,192 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:28,390 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:28,790 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:29,014 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:29,391 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:29,489 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:29,889 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:30,117 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:30,390 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:30,589 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.86 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:30,989 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.13 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:31,163 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:31,491 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:31,589 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:31,990 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.15 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:32,157 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:32,489 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:32,589 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:32,991 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:33,187 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:33,491 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:33,691 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:33,991 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:34,195 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:34,490 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:34,691 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:34,990 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.98 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:35,275 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:35,589 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:35,791 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:36,189 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:36,438 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:36,691 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:36,889 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.90 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:37,191 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:37,473 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:37,791 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:37,889 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:38,196 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:38,290 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:38,513 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:38,790 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:38,990 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.90 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:39,389 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:39,604 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:39,890 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:40,090 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:40,489 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2023-08-28 11:15:40,728 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:41,190 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:41,389 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:41,689 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:41,907 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:42,090 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:42,291 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:42,691 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.18 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:42,859 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:43,090 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:43,290 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.80 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:43,690 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:43,925 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:44,191 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:44,489 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:44,891 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:45,154 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:45,391 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:45,689 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:46,090 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:46,356 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:46,590 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:46,789 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:47,582 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:47,590 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:47,889 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:48,090 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:48,198 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:48,491 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:48,734 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:48,991 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:49,189 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:50,190 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:50,289 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:50,689 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.16 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:50,916 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:51,190 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.98 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:51,489 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:51,889 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.16 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:52,115 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:52,390 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:52,589 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:52,989 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:53,247 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:53,490 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:53,689 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:54,089 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:54,323 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:54,590 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:54,789 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:55,090 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:55,412 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:55,589 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:55,789 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:56,190 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:56,414 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:56,689 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:56,791 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:57,190 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:57,382 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:57,590 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:57,791 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.90 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:58,191 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.21 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:58,199 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.21 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:58,349 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:58,689 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:58,790 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:59,190 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.18 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:59,417 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:59,689 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:15:59,889 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:00,189 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:00,455 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:00,791 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:00,891 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.80 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:01,289 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:01,584 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:01,891 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:01,990 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:02,390 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.20 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:02,536 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:02,790 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:02,989 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:03,390 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:03,717 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:03,989 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:04,191 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.91 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:04,489 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:04,724 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:04,989 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:05,190 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:05,489 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:05,642 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:05,889 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:06,090 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:06,390 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:06,691 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:06,989 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:07,089 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:07,489 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:07,714 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:07,989 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:08,091 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:08,291 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:08,889 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:09,090 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:09,390 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:09,580 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:09,790 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:09,991 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:10,390 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:10,585 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:10,790 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:10,990 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:11,390 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.19 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:11,546 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:11,790 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:11,990 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.86 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:12,390 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.15 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:12,595 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:12,790 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:12,990 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.86 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:13,389 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:13,587 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:13,789 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:13,991 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:14,389 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:14,591 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:14,889 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:14,990 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:15,390 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:15,634 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:15,889 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:15,989 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:16,390 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:16,637 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:16,890 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:17,089 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:17,490 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:17,731 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:17,990 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:18,089 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.86 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:18,391 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.98 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:18,490 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.21 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:18,634 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:18,891 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:18,991 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:19,289 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:19,530 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:19,790 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:19,991 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:20,289 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:20,612 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:20,889 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:20,991 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.80 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:21,391 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:21,713 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:21,891 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:22,189 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:22,589 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:22,899 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:23,089 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:23,389 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.91 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:23,689 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.98 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:24,002 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:24,290 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:24,489 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:24,891 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:25,081 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:25,290 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:25,489 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.90 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:25,791 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:25,977 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:26,191 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:26,291 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:26,590 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:26,766 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:26,991 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:27,091 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:27,792 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.22 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:27,889 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.90 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:27,989 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:28,089 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:28,389 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.15 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:28,391 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.15 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:28,529 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:28,689 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:28,789 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:29,089 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:29,271 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:29,491 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:29,690 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:30,289 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:30,490 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:30,791 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.17 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:30,929 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:31,090 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:31,289 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:31,589 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.23 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:31,692 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:31,890 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:32,091 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:32,289 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:32,511 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:32,691 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:32,889 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:33,190 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.22 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:33,310 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:33,491 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:33,589 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:33,989 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.13 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:34,153 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:34,389 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:34,489 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:34,789 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:34,988 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:35,189 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:35,289 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:35,591 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:35,789 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:35,991 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:36,189 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:36,389 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:36,631 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:36,789 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:36,989 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:37,289 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.17 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:37,431 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:37,589 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:37,789 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:38,089 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.22 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:38,203 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:38,390 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:38,392 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:38,589 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:38,890 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.13 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:39,071 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:39,289 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:39,390 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:39,691 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:39,874 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:40,089 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:40,190 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:40,489 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.05 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:40,719 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:40,889 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:41,089 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:41,291 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:41,514 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:41,690 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:41,889 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:42,189 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.19 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:42,329 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:42,490 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:42,690 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.91 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:42,990 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:43,163 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:43,390 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:43,489 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:43,789 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.13 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:43,943 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:44,190 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:44,289 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.86 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:44,589 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:44,806 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:44,991 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:45,089 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:45,390 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:45,598 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:45,790 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:45,989 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:46,189 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:46,415 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:46,591 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:46,791 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:47,089 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.17 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:47,237 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:47,391 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:47,591 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:47,889 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.21 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:48,013 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:48,190 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:48,389 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:48,490 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:48,689 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.22 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:48,834 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:48,990 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:49,189 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:49,390 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:49,664 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:49,891 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:49,989 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:50,289 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:50,477 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:50,690 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:50,889 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:51,191 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.10 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:51,385 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:51,590 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:51,790 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2023-08-28 11:16:51,990 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:52,169 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:52,389 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:52,490 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:52,789 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.13 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:52,943 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:53,089 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:53,289 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:53,489 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:53,672 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:53,890 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:54,089 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2023-08-28 11:16:54,290 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:54,435 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:54,589 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:54,690 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.90 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:54,889 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:55,000 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:55,089 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:55,189 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:55,689 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:55,789 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:56,189 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:56,290 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:56,489 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.13 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:56,591 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:56,689 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:56,789 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:57,289 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:57,389 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:57,789 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:57,889 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:58,089 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.11 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:58,202 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:58,389 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:58,489 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.86 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:58,590 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.94 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:58,889 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:58,989 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:59,189 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:59,289 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:16:59,989 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:00,089 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:00,489 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:00,590 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:00,789 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.17 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:00,889 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:00,989 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:01,089 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:01,290 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:01,398 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:01,590 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:01,689 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:02,089 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:02,189 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:02,389 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.15 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:02,489 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:02,589 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:02,689 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:02,889 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:03,004 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:03,189 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:03,289 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:03,855 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:03,889 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:04,089 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:04,203 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:04,390 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:04,489 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:04,889 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:04,989 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:05,190 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.14 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:05,289 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:05,389 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:05,489 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.87 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:05,989 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:06,089 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.85 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:06,489 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:06,590 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:06,789 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.16 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:06,890 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:06,990 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.07 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:07,089 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.89 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:07,289 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:07,400 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:07,489 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.01 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:07,589 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:08,090 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:08,189 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:08,389 - distributed.worker.memory - WARNING - Worker is at 84% memory usage. Pausing worker.  Process memory: 3.13 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:08,494 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:08,590 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.98 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:08,591 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.98 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:08,690 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:09,189 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:09,289 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:09,689 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:09,789 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.90 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:09,989 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:10,093 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:10,189 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.98 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:10,289 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.82 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:10,791 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:10,890 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:11,290 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:11,389 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:11,589 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.23 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:11,689 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:11,789 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:11,889 - distributed.worker.memory - WARNING - Worker is at 77% memory usage. Resuming worker. Process memory: 2.88 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:12,090 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.04 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:12,200 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:12,889 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:12,990 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:13,189 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.22 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:13,289 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:13,389 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:13,490 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.92 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:13,690 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.06 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:13,807 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:13,990 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:14,089 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.84 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:14,289 - distributed.worker.memory - WARNING - Worker is at 86% memory usage. Pausing worker.  Process memory: 3.23 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:14,389 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:14,489 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:14,589 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.81 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:14,790 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.09 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:14,890 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:14,989 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.02 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:15,089 - distributed.worker.memory - WARNING - Worker is at 76% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:15,589 - distributed.worker.memory - WARNING - Worker is at 83% memory usage. Pausing worker.  Process memory: 3.12 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:15,689 - distributed.worker.memory - WARNING - Worker is at 75% memory usage. Resuming worker. Process memory: 2.83 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:15,890 - distributed.worker.memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 3.19 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:15,990 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:16,089 - distributed.worker.memory - WARNING - Worker is at 82% memory usage. Pausing worker.  Process memory: 3.08 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:16,190 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.91 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:16,389 - distributed.worker.memory - WARNING - Worker is at 81% memory usage. Pausing worker.  Process memory: 3.03 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:17:16,504 - distributed.worker.memory - WARNING - Worker is at 78% memory usage. Resuming worker. Process memory: 2.93 GiB -- Worker memory limit: 3.73 GiB\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2023-08-28 11:18:40,554 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.70 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:18:41,262 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 3.00 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:18:41,706 - distributed.worker.memory - WARNING - Worker is at 66% memory usage. Resuming worker. Process memory: 2.46 GiB -- Worker memory limit: 3.73 GiB\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2023-08-28 11:19:36,270 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.59 GiB -- Worker memory limit: 3.73 GiB\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2023-08-28 11:22:54,759 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.50 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:23:05,390 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:23:15,590 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:23:25,590 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:23:35,690 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:23:46,291 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:23:56,591 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:24:06,989 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:24:17,189 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2023-08-28 11:24:27,190 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:24:37,391 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:24:48,089 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2023-08-28 11:24:58,091 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.61 GiB -- Worker memory limit: 3.73 GiB\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2023-08-28 11:25:08,390 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.66 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:25:18,689 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.63 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:25:29,189 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.65 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:25:41,090 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2023-08-28 11:34:13,475 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.56 GiB -- Worker memory limit: 3.73 GiB\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2023-08-28 11:37:01,169 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.27 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:37:11,411 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.62 GiB -- Worker memory limit: 3.73 GiB\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2023-08-28 11:37:27,713 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 2.69 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:37:37,130 - distributed.worker.memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 2.99 GiB -- Worker memory limit: 3.73 GiB\n",
      "2023-08-28 11:37:37,210 - distributed.worker.memory - WARNING - Worker is at 39% memory usage. Resuming worker. Process memory: 1.49 GiB -- Worker memory limit: 3.73 GiB\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "with parallel_backend('dask'):\n",
    "    loo = LeaveOneOut()\n",
    "    scores = cross_val_score(model, x, y, cv=loo)\n",
    "\n",
    "    print(\"Leave One Out CV Scores:\", scores)\n",
    "    print(\"Average Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:184: PerformanceWarning: Slicing with an out-of-order index is generating 386414 times more chunks\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:184: PerformanceWarning: Slicing with an out-of-order index is generating 386478 times more chunks\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:184: PerformanceWarning: Slicing with an out-of-order index is generating 386336 times more chunks\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:184: PerformanceWarning: Slicing with an out-of-order index is generating 386776 times more chunks\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:184: PerformanceWarning: Slicing with an out-of-order index is generating 128814 times more chunks\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:184: PerformanceWarning: Slicing with an out-of-order index is generating 128786 times more chunks\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:184: PerformanceWarning: Slicing with an out-of-order index is generating 128679 times more chunks\n",
      "  return array[key] if axis == 0 else array[:, key]\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py:184: PerformanceWarning: Slicing with an out-of-order index is generating 128798 times more chunks\n",
      "  return array[key] if axis == 0 else array[:, key]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "with parallel_backend('dask'):\n",
    "    ss = ShuffleSplit(n_splits=5, test_size=0.25)\n",
    "    scores = cross_val_score(model, x, y, cv=ss)\n",
    "\n",
    "    print(\"Shuffle Split CV Scores:\", scores)\n",
    "    print(\"Average Score:\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/gea/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group K-Fold CV Scores: [0.1632969  0.16233749 0.16436082 0.16342331 0.16350308]\n",
      "Average Score: 0.1633843212001469\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "with parallel_backend('dask'):\n",
    "# Для этого примера, мы будем использовать группы как дополнительный массив.\n",
    "# Это может не быть совсем релевантным для датасета Iris без реальных групп.\n",
    "    groups = np.arange(len(x)) // 10  # Просто пример группировки\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    scores = cross_val_score(model, x, y, groups=groups, cv=gkf)\n",
    "\n",
    "    print(\"Group K-Fold CV Scores:\", scores)\n",
    "    print(\"Average Score:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
