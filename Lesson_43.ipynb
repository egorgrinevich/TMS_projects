{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a3d25b-834b-4655-be2e-a0c649a5db90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.0/en_core_web_md-3.7.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /home/gea/anaconda3/lib/python3.11/site-packages (from en-core-web-md==3.7.0) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/gea/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/gea/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gea/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/gea/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gea/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gea/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/gea/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/gea/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/gea/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/gea/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gea/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->en-core-web-md==3.7.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "# !pip install gensim nltk spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f122552f-d5f9-4801-a4f4-5ff2c11582bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8ac0190-414d-4048-b43c-7717206916c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sidromnik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sidromnik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sidromnik\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст:\n",
      "Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language.\n",
      "\n",
      "Токенизированный текст:\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.']\n",
      "\n",
      "Текст без стоп-слов:\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'subfield', 'artificial', 'intelligence', 'focuses', 'interaction', 'computers', 'humans', 'natural', 'language', '.']\n",
      "\n",
      "Лемматизированный текст:\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'subfield', 'artificial', 'intelligence', 'focus', 'interaction', 'computer', 'human', 'natural', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "# Предварительная загрузка ресурсов NLTK (если не установлено)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Пример текста для обработки\n",
    "text = \"Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language.\"\n",
    "\n",
    "# Токенизация: разделение текста на слова\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Удаление стоп-слов: слова, которые часто встречаются и не несут смысла (предлоги, союзы и др.)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Лемматизация: приведение слов к их базовой форме (лемме)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "# Результаты\n",
    "print(\"Исходный текст:\")\n",
    "print(text)\n",
    "\n",
    "print(\"\\nТокенизированный текст:\")\n",
    "print(tokens)\n",
    "\n",
    "print(\"\\nТекст без стоп-слов:\")\n",
    "print(filtered_tokens)\n",
    "\n",
    "print(\"\\nЛемматизированный текст:\")\n",
    "print(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc0230c-93cc-4d30-8e53-16d5e5cd19e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32488162-601a-4728-8e52-87f238403a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторное представление слова 'language':\n",
      "[ 9.4563962e-05  3.0773198e-03 -6.8126451e-03 -1.3754654e-03\n",
      "  7.6685809e-03  7.3464094e-03 -3.6732971e-03  2.6427018e-03\n",
      " -8.3171297e-03  6.2054861e-03 -4.6373224e-03 -3.1641065e-03\n",
      "  9.3113566e-03  8.7338570e-04  7.4907029e-03 -6.0740625e-03\n",
      "  5.1605068e-03  9.9228229e-03 -8.4573915e-03 -5.1356913e-03\n",
      " -7.0648370e-03 -4.8626517e-03 -3.7785638e-03 -8.5361991e-03\n",
      "  7.9556061e-03 -4.8439382e-03  8.4236134e-03  5.2625705e-03\n",
      " -6.5500261e-03  3.9578713e-03  5.4701497e-03 -7.4265362e-03\n",
      " -7.4057197e-03 -2.4752307e-03 -8.6257253e-03 -1.5815723e-03\n",
      " -4.0343284e-04  3.2996845e-03  1.4418805e-03 -8.8142155e-04\n",
      " -5.5940580e-03  1.7303658e-03 -8.9737179e-04  6.7936908e-03\n",
      "  3.9735902e-03  4.5294715e-03  1.4343059e-03 -2.6998555e-03\n",
      " -4.3668128e-03 -1.0320747e-03  1.4370275e-03 -2.6460087e-03\n",
      " -7.0737829e-03 -7.8053069e-03 -9.1217868e-03 -5.9351693e-03\n",
      " -1.8474245e-03 -4.3238713e-03 -6.4606704e-03 -3.7173224e-03\n",
      "  4.2891586e-03 -3.7390434e-03  8.3781751e-03  1.5339935e-03\n",
      " -7.2423196e-03  9.4337985e-03  7.6312125e-03  5.4932819e-03\n",
      " -6.8488456e-03  5.8226790e-03  4.0090932e-03  5.1853694e-03\n",
      "  4.2559016e-03  1.9397545e-03 -3.1701624e-03  8.3538452e-03\n",
      "  9.6121803e-03  3.7926030e-03 -2.8369951e-03  7.1275235e-06\n",
      "  1.2188185e-03 -8.4583247e-03 -8.2239453e-03 -2.3101569e-04\n",
      "  1.2372875e-03 -5.7433806e-03 -4.7252737e-03 -7.3460746e-03\n",
      "  8.3286157e-03  1.2129784e-04 -4.5093987e-03  5.7017053e-03\n",
      "  9.1800150e-03 -4.0998720e-03  7.9646818e-03  5.3754342e-03\n",
      "  5.8791232e-03  5.1259040e-04  8.2130842e-03 -7.0190406e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sidromnik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "# Пример текстового корпуса для обучения модели Word2Vec\n",
    "corpus = [\n",
    "    \"Natural Language Processing (NLP) is a subfield of artificial intelligence.\",\n",
    "    \"It focuses on the interaction between computers and humans through natural language.\",\n",
    "    \"NLP has many applications, including sentiment analysis and machine translation.\"\n",
    "]\n",
    "\n",
    "# Токенизация текста\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "# Обучение модели Word2Vec\n",
    "model = Word2Vec(tokenized_corpus, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Получение векторного представления слова\n",
    "vector = model.wv['language']\n",
    "print(\"Векторное представление слова 'language':\")\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3974a865-4eec-4872-b4e4-69869835d6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторное представление слова 'Language':\n",
      "[-2.0082   -4.1421    3.1264    4.0433    2.0183   -2.1069    4.8858\n",
      "  0.81069  -2.8429   -1.5438    4.9073    1.1409   -4.302     3.9497\n",
      " -0.86569  -0.71589   7.2681    2.1416   -2.876    -0.98953  -2.319\n",
      "  4.378    -3.1976    1.734    -0.23881  -1.9915   -0.18745  -2.1277\n",
      "  0.70861  -0.80543   4.3059   -0.077441 -0.53142   0.57004  -0.95075\n",
      " -5.0915    4.0665    1.1612    0.43608   0.082274  2.3584   -1.1396\n",
      " -0.012666  0.80816  -5.5515    2.5399    2.4488    0.51033  -1.2908\n",
      " -5.0774   -1.206     1.4263   -1.7982   -2.8735   -1.5467    3.3573\n",
      " -4.5017    0.48971   2.3641   -1.0195    3.9824    1.0017   -1.6755\n",
      "  2.5672    2.8473   -1.228    -1.6405   -2.9723   -0.41897   0.90382\n",
      " -2.4969    0.28523  -3.5221    4.0404   -2.4785   -0.68507  -4.2178\n",
      "  4.3244    1.036     1.0949   -2.7989   -2.0504   -0.33526   2.7463\n",
      " -2.268     0.034702 -1.8862   -0.56404   0.56816   1.371     1.1729\n",
      "  2.4573    0.089652 -0.92281  -2.2505   -0.50528  -0.95277   0.34163\n",
      "  1.5353    0.7731    2.1685    0.049115  1.1902    1.332    -2.3894\n",
      "  4.8155    1.2037   -1.8714    2.253    -0.79581   2.4503   -0.8676\n",
      " -2.8439   -0.58226   3.9211    2.6038   -0.72829   0.73667  -5.372\n",
      " -2.4721   -2.7937   -2.0813    2.3254   -1.0752   -0.10638  -5.3361\n",
      " -0.68906  -5.4331    3.7046   -1.7445   -3.2418    5.2622    5.1305\n",
      "  0.81229  -1.4199   -2.0933   -1.2184   -3.7514    0.66114   1.5209\n",
      "  1.4648   -0.40889  -1.9868    1.6288    0.62792   0.089333 -3.3623\n",
      "  0.66547   4.7587    3.162     0.99324   0.51903  -1.1268    0.74386\n",
      " -1.1817    0.095622 -0.70612   1.1078   -0.46749  -5.1911    1.2298\n",
      " -0.11662   0.088663  2.7336   -0.05091  -1.4494   -3.8339   -1.6605\n",
      "  3.3373    2.0523   -5.2742   -1.8986   -1.1577   -1.3453    0.60428\n",
      "  3.1076    4.548    -0.95762  -5.4069    0.25396   1.0754    1.0098\n",
      "  1.1954   -1.1329   -2.2191    2.1313   -1.022     1.2068   -2.826\n",
      "  2.1608    0.27561  -1.1371    0.30403  -1.6979    3.7362    3.4886\n",
      " -5.4255   -0.46637   3.4097   -2.1991   -0.083915 -0.7456    0.98387\n",
      " -1.0878    3.094    -0.13546  -5.1922    1.0125    2.9052   -0.43378\n",
      " -2.4586    2.4173   -1.0116    0.095896  1.0278    1.2755    6.1474\n",
      " -2.0821    2.0704   -0.13942   0.11896   2.3498   -1.1141   -1.9005\n",
      " -1.4771   -1.52      1.6943    1.2186   -0.56333   0.87627   2.548\n",
      "  1.8357    0.41441   1.9709    3.1334   -0.77358   2.6188   -1.2899\n",
      " -1.7764   -0.13763  -3.3011    0.10174   1.4399    0.94837  -2.5394\n",
      "  1.5439   -2.5526    1.5605   -1.9545   -1.1842    1.3439   -3.0522\n",
      " -5.3501   -2.7115    3.3011   -2.7367    2.293    -0.014553  0.4094\n",
      " -0.56376   0.45921   4.506     3.8514    1.4799   -0.42911  -2.0219\n",
      " -1.7667    3.1802    0.71687  -2.3897   -0.1081   -0.77949   5.7833\n",
      " -0.43125   1.6393   -2.0158   -3.4128   -3.2206    1.8668    0.39165\n",
      "  2.9403   -0.63692   0.47244  -2.3371   -1.5978   -3.3941   -0.1552\n",
      "  0.5723   -0.82468  -0.15958   0.082664 -0.49039  -0.95866  -3.3204\n",
      " -1.1827    1.5196    2.0029   -4.7463    0.1578    1.3826  ]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Загрузка модели GloVe (предварительно обученной модели GloVe)\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Пример текста для анализа\n",
    "text = \"Natural Language Processing (NLP) is a subfield of artificial intelligence.\"\n",
    "\n",
    "# Анализ текста с использованием модели GloVe\n",
    "doc = nlp(text)\n",
    "\n",
    "# Получение векторного представления слова\n",
    "vector = doc[2].vector  # \"Language\"\n",
    "print(\"Векторное представление слова 'Language':\")\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715661c7-2f76-4984-a8ce-2cb47208d544",
   "metadata": {},
   "source": [
    "Преобразование TF-IDF (Term Frequency-Inverse Document Frequency) используется для оценки важности слов в текстовых документах. Этот метод выделяет слова, которые встречаются часто в конкретном документе, но редко в других документах, считая их более информативными. Вот пример TF-IDF преобразования на Python с использованием библиотеки scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2f024b2-ba4e-4bb9-a990-f70205ec236e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF матрица:\n",
      "[[0.         0.         0.         0.         0.38086517 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.38086517 0.         0.30027847 0.         0.2431013  0.\n",
      "  0.         0.30027847 0.30027847 0.30027847 0.         0.\n",
      "  0.38086517 0.         0.38086517 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.24599227 0.         0.         0.         0.31201\n",
      "  0.31201    0.31201    0.         0.         0.31201    0.\n",
      "  0.         0.31201    0.         0.31201    0.19915194 0.\n",
      "  0.         0.24599227 0.         0.         0.31201    0.\n",
      "  0.         0.         0.         0.         0.         0.24599227\n",
      "  0.31201    0.         0.         0.        ]\n",
      " [0.34327249 0.27063997 0.         0.34327249 0.         0.\n",
      "  0.         0.         0.         0.34327249 0.         0.34327249\n",
      "  0.         0.         0.         0.         0.         0.27063997\n",
      "  0.34327249 0.         0.27063997 0.         0.         0.\n",
      "  0.         0.34327249 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.27063997]\n",
      " [0.         0.         0.30838102 0.         0.         0.\n",
      "  0.         0.         0.30838102 0.         0.         0.\n",
      "  0.         0.         0.24313114 0.         0.19683561 0.24313114\n",
      "  0.         0.         0.         0.24313114 0.         0.30838102\n",
      "  0.         0.         0.         0.30838102 0.30838102 0.24313114\n",
      "  0.         0.30838102 0.30838102 0.24313114]]\n",
      "\n",
      "Список слов (терминов):\n",
      "['analysis' 'and' 'another' 'applications' 'artificial' 'between'\n",
      " 'computers' 'focuses' 'from' 'has' 'humans' 'including' 'intelligence'\n",
      " 'interaction' 'is' 'it' 'language' 'machine' 'many' 'natural' 'nlp' 'of'\n",
      " 'on' 'one' 'processing' 'sentiment' 'subfield' 'task' 'text' 'the'\n",
      " 'through' 'to' 'translating' 'translation']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Пример текстовых документов\n",
    "documents = [\n",
    "    \"Natural Language Processing (NLP) is a subfield of artificial intelligence.\",\n",
    "    \"It focuses on the interaction between computers and humans through natural language.\",\n",
    "    \"NLP has many applications, including sentiment analysis and machine translation.\",\n",
    "    \"Machine translation is the task of translating text from one language to another.\",\n",
    "]\n",
    "\n",
    "# Создание объекта TF-IDF векторизатора\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Преобразование текстовых документов в TF-IDF векторы\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Получение списка слов (терминов) в порядке их индексов\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Преобразование TF-IDF матрицы в массив NumPy и вывод результата\n",
    "tfidf_matrix_array = tfidf_matrix.toarray()\n",
    "print(\"TF-IDF матрица:\")\n",
    "print(tfidf_matrix_array)\n",
    "\n",
    "# Вывод списка слов\n",
    "print(\"\\nСписок слов (терминов):\")\n",
    "print(terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74f13fb5-41cb-4c25-923e-f9d1218ec32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sidromnik\\.conda\\envs\\NLP\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.89      0.90      4961\n",
      "    positive       0.89      0.91      0.90      5039\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n",
      "[[4403  558]\n",
      " [ 440 4599]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Загрузка данных из CSV-файла\n",
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['review'], data['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Преобразование текстовых данных в векторы TF-IDF\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "# Обучение модели логистической регрессии\n",
    "clf = LogisticRegression(max_iter=100)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Преобразование тестовых данных в векторы TF-IDF\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "# Предсказание меток для тестовых данных\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# Оценка точности модели\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Вывод отчета о классификации и матрицы ошибок\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27bb2c4-ad47-440b-a900-ad205185dc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27548b06-bb06-4d25-a7cf-b300ea8e4056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4914ed8-00a6-4d33-8efd-02f798a02519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b49399-5971-474c-850f-1dbcb66db923",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
