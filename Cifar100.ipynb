{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa1c956a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 19:21:29.574313: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-29 19:21:29.786674: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-29 19:21:29.788650: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-29 19:21:30.845176: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import numpy as np\n",
    "from keras.datasets import cifar100\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52cf9401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(tf.config.list_physical_devices('GPU'))\n",
    "else:\n",
    "    print(\"TensorFlow GPU not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca162fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d87cefc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2. 4. 6.], shape=(3,), dtype=float32)\n",
      "CPU times: user 362 ms, sys: 191 ms, total: 553 ms\n",
      "Wall time: 901 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.device('/GPU:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0], shape=[3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0], shape=[3], name='b')\n",
    "    c = tf.add(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64cc9cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2. 4. 6.], shape=(3,), dtype=float32)\n",
      "CPU times: user 0 ns, sys: 1.96 ms, total: 1.96 ms\n",
      "Wall time: 1.33 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.device('/CPU:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0], shape=[3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0], shape=[3], name='b')\n",
    "    c = tf.add(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a8e1f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка CIFAR-10 датасета\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar100.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8aca209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализация изображений\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Преобразование меток в one-hot encoding\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4760ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (10000, 32, 32, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape, test_images.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e31511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание объекта ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77026515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выполнение аугментации данных с использованием datagen\n",
    "datagen.fit(train_images)\n",
    "\n",
    "# Пример использования .flow() для обучения модели\n",
    "# model.fit(datagen.flow(train_images, train_labels, batch_size=32), steps_per_epoch=len(train_images) / 32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ba98ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Генерация аугментированных изображений\n",
    "augmented_images = [train_images[1] for i in range(5)]\n",
    "augmented_images = np.array(augmented_images)\n",
    "augmented_images = datagen.flow(augmented_images, batch_size=5, shuffle=False).next()\n",
    "\n",
    "# Визуализация изображений\n",
    "fig, axarr = plt.subplots(1, 5, figsize=(20, 5))\n",
    "\n",
    "for i in range(5):\n",
    "    axarr[i].imshow(augmented_images[i])\n",
    "    axarr[i].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "876bbacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f07d139b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_40 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_40 (MaxPooli  (None, 16, 16, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (None, 15, 15, 512)       66048     \n",
      "                                                                 \n",
      " max_pooling2d_41 (MaxPooli  (None, 7, 7, 512)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_42 (Conv2D)          (None, 5, 5, 512)         2359808   \n",
      "                                                                 \n",
      " max_pooling2d_42 (MaxPooli  (None, 1, 1, 512)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2583780 (9.86 MB)\n",
      "Trainable params: 2583780 (9.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Определение структуры модели\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), padding = \"same\", activation='relu', strides = 1, input_shape=(32, 32, 3)), # Сверточный слой с 32 фильтрами\n",
    "    MaxPooling2D((2, 2)), # Слой пулинга\n",
    "    Dropout(0.1),\n",
    "    \n",
    "    Conv2D(512, (2, 2), activation='relu'), # Следующий сверточный слой с 64 фильтрами\n",
    "    MaxPooling2D((2, 2)), # Слой пулинга\n",
    "    \n",
    "    Conv2D(512, (3, 3), activation='relu'), # Следующий сверточный слой с 64 фильтрами\n",
    "    MaxPooling2D((3, 3)), # Слой пулинга\n",
    "    # Conv2D(256, (3, 3), activation='relu'), # Еще один сверточный слой с 64 фильтрами\n",
    "    Flatten(), # Преобразование матрицы признаков в вектор\n",
    "    Dense(256, activation='relu'), # Полносвязный слой\n",
    "    \n",
    "    # MaxPooling2D((2, 2), strides=2),\n",
    "    # Conv2D(32, (3,3), padding='same', activation='relu'),\n",
    "    # MaxPooling2D((2, 2), strides=2),\n",
    "    # Dropout(0.1),\n",
    "    # Conv2D(64, (3,3), padding='same', activation='relu'),\n",
    "    # MaxPooling2D((2, 2), strides=2),\n",
    "    # Dropout(0.15),\n",
    "    # Conv2D(128, (3,3), padding='same', activation='relu'),\n",
    "    # MaxPooling2D((2, 2), strides=2),\n",
    "    # Dropout(0.2),\n",
    "    # Conv2D(512, (3,3), padding='same', activation='relu'),\n",
    "    # MaxPooling2D((2, 2), strides=2),\n",
    "    # Dropout(0.25), \n",
    "    # Flatten(),\n",
    "    # Dense(64, activation='relu'),\n",
    "    # Dropout(0.2),\n",
    "    # Dense(32, activation='relu'),\n",
    "    # Dropout(0.1),\n",
    "    \n",
    "    Dense(100, activation='softmax') # Выходной слой с 10 нейронами (по количеству классов в CIFAR-10)\n",
    "])\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4d58f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 151/1562 [=>............................] - ETA: 7:29 - loss: 4.4464 - accuracy: 0.0268"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gea/TMS_projects/Cifar100.ipynb Ячейка 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gea/TMS_projects/Cifar100.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Обучение модели\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/gea/TMS_projects/Cifar100.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(datagen\u001b[39m.\u001b[39;49mflow(train_images, train_labels, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gea/TMS_projects/Cifar100.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m           steps_per_epoch\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(train_images) \u001b[39m/\u001b[39;49m \u001b[39m32\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gea/TMS_projects/Cifar100.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m           epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/gea/TMS_projects/Cifar100.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m           validation_data\u001b[39m=\u001b[39;49m(test_images, test_labels))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Обучение модели\n",
    "model.fit(datagen.flow(train_images, train_labels, batch_size=64),\n",
    "          steps_per_epoch=len(train_images) / 32,\n",
    "          epochs=10,\n",
    "          validation_data=(test_images, test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcedddf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Выберем одно изображение для визуализации признаков\n",
    "img = train_images[0]\n",
    "img = np.expand_dims(img, axis=0)  # расширим измерения для соответствия входному формату модели\n",
    "\n",
    "# Получим выходные данные каждого слоя для выбранного изображения\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(img)\n",
    "\n",
    "# Функция для интерполяции изображений до размера 128x128\n",
    "def upscale_to_128x128(img):\n",
    "    return np.resize(img, (128, 128))\n",
    "\n",
    "# Визуализация признаков\n",
    "for layer_name, layer_activation in zip([layer.name for layer in model.layers], activations):\n",
    "    # Если активация имеет 4 измерения (как в случае с Conv2D и MaxPooling2D)\n",
    "    if len(layer_activation.shape) == 4:\n",
    "        # Выберем первые 6 признаковых карт для визуализации\n",
    "        for i in range(6):\n",
    "            plt.figure(figsize=(16, 4))\n",
    "            \n",
    "            # Некоторые слои могут иметь менее 6 каналов, в таком случае мы визуализируем только имеющиеся\n",
    "            if layer_activation.shape[-1] >= i+1:\n",
    "                channel_image = layer_activation[0, :, :, i]\n",
    "                channel_image = upscale_to_128x128(channel_image)  # Интерполяция до размера 128x128\n",
    "                plt.imshow(channel_image, cmap='viridis')\n",
    "                plt.title(f\"Features from layer: {layer_name}\")\n",
    "                plt.axis('off')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71634ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1562/1562 [==============================] - 20s 12ms/step - loss: 1.6416 - accuracy: 0.3975 - val_loss: 1.3410 - val_accuracy: 0.5207\n",
      "Epoch 2/10\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 1.3038 - accuracy: 0.5337 - val_loss: 1.1359 - val_accuracy: 0.5960\n",
      "Epoch 3/10\n",
      "1562/1562 [==============================] - 21s 13ms/step - loss: 1.1926 - accuracy: 0.5743 - val_loss: 1.0617 - val_accuracy: 0.6317\n",
      "Epoch 4/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 1.1299 - accuracy: 0.5998 - val_loss: 1.0259 - val_accuracy: 0.6410\n",
      "Epoch 5/10\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 1.0769 - accuracy: 0.6181 - val_loss: 1.0082 - val_accuracy: 0.6480\n",
      "Epoch 6/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 1.0332 - accuracy: 0.6364 - val_loss: 0.9479 - val_accuracy: 0.6680\n",
      "Epoch 7/10\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.9866 - accuracy: 0.6513 - val_loss: 0.9128 - val_accuracy: 0.6870\n",
      "Epoch 8/10\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.9676 - accuracy: 0.6593 - val_loss: 0.8866 - val_accuracy: 0.6936\n",
      "Epoch 9/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.9451 - accuracy: 0.6663 - val_loss: 0.8755 - val_accuracy: 0.6971\n",
      "Epoch 10/10\n",
      "1562/1562 [==============================] - 21s 13ms/step - loss: 0.9261 - accuracy: 0.6742 - val_loss: 0.8571 - val_accuracy: 0.7017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f207c2571c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Определение структуры модели\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)), # Сверточный слой с 32 фильтрами\n",
    "    MaxPooling2D((2, 2)), # Слой пулинга\n",
    "    Conv2D(64, (3, 3), activation='relu'), # Следующий сверточный слой с 64 фильтрами\n",
    "    MaxPooling2D((2, 2)), # Слой пулинга\n",
    "    Conv2D(64, (3, 3), activation='relu'), # Еще один сверточный слой с 64 фильтрами\n",
    "    Flatten(), # Преобразование матрицы признаков в вектор\n",
    "    Dense(64, activation='relu'), # Полносвязный слой\n",
    "    Dense(10, activation='softmax') # Выходной слой с 10 нейронами (по количеству классов в CIFAR-10)\n",
    "])\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Определение каллбека\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Обучение модели\n",
    "model.fit(datagen.flow(train_images, train_labels, batch_size=32),\n",
    "          steps_per_epoch=len(train_images) / 32,\n",
    "          epochs=10,\n",
    "          validation_data=(test_images, test_labels),\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2c0bc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1562/1562 [==============================] - 21s 13ms/step - loss: 1.6333 - accuracy: 0.4014 - val_loss: 1.3017 - val_accuracy: 0.5280 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 1.3047 - accuracy: 0.5328 - val_loss: 1.1562 - val_accuracy: 0.5879 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 1.1825 - accuracy: 0.5795 - val_loss: 1.0093 - val_accuracy: 0.6397 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 1.1079 - accuracy: 0.6035 - val_loss: 0.9927 - val_accuracy: 0.6503 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1562/1562 [==============================] - 21s 14ms/step - loss: 1.0552 - accuracy: 0.6260 - val_loss: 0.9771 - val_accuracy: 0.6649 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 1.0075 - accuracy: 0.6452 - val_loss: 0.8838 - val_accuracy: 0.6937 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.9674 - accuracy: 0.6617 - val_loss: 0.9503 - val_accuracy: 0.6689 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1562/1562 [==============================] - 19s 12ms/step - loss: 0.9400 - accuracy: 0.6697 - val_loss: 0.8800 - val_accuracy: 0.6991 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.9163 - accuracy: 0.6772 - val_loss: 0.8740 - val_accuracy: 0.6902 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1562/1562 [==============================] - 22s 14ms/step - loss: 0.9019 - accuracy: 0.6845 - val_loss: 0.8369 - val_accuracy: 0.7115 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f207c087d30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Определение структуры модели\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)), # Сверточный слой с 32 фильтрами\n",
    "    MaxPooling2D((2, 2)), # Слой пулинга\n",
    "    Conv2D(64, (3, 3), activation='relu'), # Следующий сверточный слой с 64 фильтрами\n",
    "    MaxPooling2D((2, 2)), # Слой пулинга\n",
    "    Conv2D(64, (3, 3), activation='relu'), # Еще один сверточный слой с 64 фильтрами\n",
    "    Flatten(), # Преобразование матрицы признаков в вектор\n",
    "    Dense(64, activation='relu'), # Полносвязный слой\n",
    "    Dense(10, activation='softmax') # Выходной слой с 10 нейронами (по количеству классов в CIFAR-10)\n",
    "])\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Определение каллбеков\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_lr=1e-6)\n",
    "\n",
    "# Обучение модели\n",
    "model.fit(datagen.flow(train_images, train_labels, batch_size=32),\n",
    "          steps_per_epoch=len(train_images) / 32,\n",
    "          epochs=10,\n",
    "          validation_data=(test_images, test_labels),\n",
    "          callbacks=[early_stopping, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09154870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1562/1562 [==============================] - 21s 13ms/step - loss: 1.6107 - accuracy: 0.4075 - val_loss: 1.3518 - val_accuracy: 0.5006 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 1.3233 - accuracy: 0.5257 - val_loss: 1.1354 - val_accuracy: 0.5988 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 1.1981 - accuracy: 0.5732 - val_loss: 1.0972 - val_accuracy: 0.6115 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 1.1250 - accuracy: 0.6005 - val_loss: 1.0187 - val_accuracy: 0.6416 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 1.0732 - accuracy: 0.6192 - val_loss: 0.9965 - val_accuracy: 0.6542 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 1.0368 - accuracy: 0.6347 - val_loss: 0.9064 - val_accuracy: 0.6849 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 1.0022 - accuracy: 0.6437 - val_loss: 0.9316 - val_accuracy: 0.6759 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.9755 - accuracy: 0.6568 - val_loss: 0.9030 - val_accuracy: 0.6907 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.9545 - accuracy: 0.6641 - val_loss: 0.8771 - val_accuracy: 0.6918 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "1562/1562 [==============================] - 20s 13ms/step - loss: 0.9335 - accuracy: 0.6715 - val_loss: 0.9028 - val_accuracy: 0.6871 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2075f421c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "import datetime\n",
    "\n",
    "# Определение структуры модели\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)), # Сверточный слой с 32 фильтрами\n",
    "    MaxPooling2D((2, 2)), # Слой пулинга\n",
    "    Conv2D(64, (3, 3), activation='relu'), # Следующий сверточный слой с 64 фильтрами\n",
    "    MaxPooling2D((2, 2)), # Слой пулинга\n",
    "    Conv2D(64, (3, 3), activation='relu'), # Еще один сверточный слой с 64 фильтрами\n",
    "    Flatten(), # Преобразование матрицы признаков в вектор\n",
    "    Dense(64, activation='relu'), # Полносвязный слой\n",
    "    Dense(10, activation='softmax') # Выходной слой с 10 нейронами (по количеству классов в CIFAR-10)\n",
    "])\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Определение каллбеков\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_lr=1e-6)\n",
    "\n",
    "# Добавление TensorBoard callback\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Обучение модели\n",
    "model.fit(datagen.flow(train_images, train_labels, batch_size=32),\n",
    "          steps_per_epoch=len(train_images) / 32,\n",
    "          epochs=10,\n",
    "          validation_data=(test_images, test_labels),\n",
    "          callbacks=[early_stopping, reduce_lr, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e553b72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 4002), started 3:01:50 ago. (Use '!kill 4002' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d8a8b5b50cf4df3b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d8a8b5b50cf4df3b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c34c3b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint\n",
    "import datetime\n",
    "\n",
    "# Определение структуры модели\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)), # Сверточный слой с 32 фильтрами\n",
    "    MaxPooling2D((2, 2)), # Слой пулинга\n",
    "    Conv2D(64, (3, 3), activation='relu'), # Следующий сверточный слой с 64 фильтрами\n",
    "    MaxPooling2D((2, 2)), # Слой пулинга\n",
    "    Conv2D(64, (3, 3), activation='relu'), # Еще один сверточный слой с 64 фильтрами\n",
    "    Flatten(), # Преобразование матрицы признаков в вектор\n",
    "    Dense(64, activation='relu'), # Полносвязный слой\n",
    "    Dense(10, activation='softmax') # Выходной слой с 10 нейронами (по количеству классов в CIFAR-10)\n",
    "])\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Определение каллбеков\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_lr=1e-6)\n",
    "\n",
    "# Добавление TensorBoard callback\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Добавление ModelCheckpoint callback\n",
    "best_model_path = \"best_model.h5\"\n",
    "checkpoint_callback = ModelCheckpoint(best_model_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# Обучение модели\n",
    "model.fit(datagen.flow(train_images, train_labels, batch_size=32),\n",
    "          steps_per_epoch=len(train_images) / 32,\n",
    "          epochs=10,\n",
    "          validation_data=(test_images, test_labels),\n",
    "          callbacks=[early_stopping, reduce_lr, tensorboard_callback,checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2036023f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970e1111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c42fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976be8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7981306e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08ad514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f387cf40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2248790e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
